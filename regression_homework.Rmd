---
title: "Regression"
author: "Anna Yeaton"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

#Due Thursday Sept. 26th

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)
```

# Lab Section

In this lab, we will go over regression. We will be using the caret package in R. https://topepo.github.io/caret/train-models-by-tag.html

# Perfomance Metrics 

## Residual 

Deviation of the observed value to the estimated value (sample mean)
$$residual=y_i - \hat{y_i}$$
where $\hat{y_i}$ is the estimated value

## Mean Squared Error (MSE)

$$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$$

## Root Mean Squared Error (RMSE)
Same units as original data.

$$RMSE=\sqrt{MSE}$$

## L2 regularization : Ridge regression. Regularize by adding the sum of the coefficients, squared, to the function. 

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$

## L1 regularization : Lasso Regression. Regularize by adding the sum of the absolute value of the coefficients to the model. Coefficient estimates may be pushed to zero -- Lasso can perform variable selection

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$


\newpage

### The broad steps of Machine learning in R. 

1. Split the data into training and test. Set test aside. 

2. Fit a good model to the training data. 

3. See how your model did on the training data.

4. Test how your model performs on the test data. 

# Regression

```{r, include=FALSE}
library(caret)
library(MASS)
library(ggplot2)
library(dplyr)
library(ggfortify)


#Mauna Loa CO2 concentrations
airq <-airquality
```


1. Split data into training and test set (75% in train set, 25% in test set)

```{r}
set.seed(123)  
smp_siz <- floor(0.75*nrow(airq))
part <- sample(seq(nrow(airq)),size = smp_siz)
train_regression <- airq[part,]
test_regression <- airq[-part,]

```

### Linear Regression

* Assumes a linear relationship. 
* Independent variables should not be correlated (no mulitcollinearity)
* The number of observations should be greater than the number of independent variables.


$$RSS=\sum(y_i - \hat{y_i})^2$$
We will predict the response of the Temperature based on Wind. 

This is the data we will fit a linear model to. 
```{r}
ggplot(data = train_regression) +
  geom_point(aes(x=Wind, y=Temp)) +
  theme_bw()
```

2. Create and fit a linear model to predict Temperature from Wind using the training set

```{r}
#help(train)
#airq
#linear_regression <- train( ~ , data= , method = "lm")
linreg <- train(Temp ~ Wind, train_regression, method = "lm")
fit <- lm(airq$Temp ~ airq$Wind)
```


3. Vizualize how your model performed on the train data by plotting the regression line on top of the train data points. 
```{r}

#use the ggplot created in the previous chunk

ggplot(data = train_regression) +
  geom_point(aes(x=Wind, y=Temp)) +
  theme_bw()+
  geom_abline(slope = fit$coefficients[2], intercept = fit$coefficients[1], color = 'red')

# plot(airq$Wind, airq$Temp, pch = 1)
# abline(fit$coefficients[1],fit$coefficients[2],col='red')
# 
# plot(linreg$trainingData)
# abline(fit$coefficients[1], fit$coefficients[2], col = 'red')
# 
# plot(train_regression$Temp)
# abline(fit$coefficients[1], fit$coefficients[2], col = 'red')


```


4. Explore how the model performs on the test data. For Linear Regression:

* The residuals should be close to zero.
* There should be equal variance around the regression line (homoscedasticity).
* Residuals should be normally distributed.
* Independent variables and residuals should not be correlated.

4 a) See how the model performs on the test data
```{r}
#help(predict)
#linear_predict <- predict(, newdata=)

linpred <- predict(linreg, test_regression)
length(linpred)
```

4 b) Look at the residuals. Are they close to zero?
```{r}
#look at the median residual value. Close to zero is best
#help(summary)
resi <- resid(linreg)
#they are not close to 0
```


4 c) Plot predicted temperature vs observed temperature. A strong model should show a strong correlation
```{r}
pred.obs <- cbind(test_regression, linpred)
plot(linpred, test_regression$Temp, xlab = "predicted", ylab = "observed")
ggplot(data = pred.obs)+
  geom_point(mapping = aes(linpred, test_regression$Temp))+
  ggtitle("Predicted Temperature vs Observed Temperature")+
  labs(x = "Predicted", y = "Observed")
cor.test(test_regression$Temp, linpred)
```

4 d) Visualize the predicted values in relation to the real data points. Look for homoscedasticity
```{r}
# Extract coefficients from the model
fit$coefficients[1]
fit$coefficients[2]
# plot the regression line on the predicted values
plot(test_regression$Temp, linpred, xlab = "observed", ylab = "predicted")
abline(fit$coefficients[1],fit$coefficients[2],col='red')
# plot the original test values

plot(test_regression$Temp, linpred, xlab = "observed", ylab = "predicted")
abline(fit$coefficients[1],fit$coefficients[2],col='red')
length(linpred)

#REVIEW THIS SHIT

ggplot(pred.obs, aes(x = Wind , y = Temp)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = Wind, yend = linpred), alpha = .2) +
  geom_point(aes(color = abs(linpred))) + 
  scale_color_continuous(low = "black", high = "red") +  
  guides(color = FALSE) +  
  geom_point(aes(y = linpred), shape = 1) +
  theme_bw()
```

4 e) Residuals should be normally distributed. Plot the density of the residuals
```{r}

#residuals_lin <- residuals(linear_regression)
resi_lin <- residuals(linreg)
data <- cbind(train_regression, resi_lin)
#ggplot(data=residvpredict) +
#  geom_density(aes(residual))

ggplot(data, aes(x=resi_lin)) + 
  geom_density()+
  ggtitle("Residual Density")
  

```


4 f) Independent variables and residuals should not be correlated
```{r}
#cor.test(train_regression$Wind, resid(linear_regression))
cor.test(train_regression$Wind, resid(linreg))
length(train_regression$Wind)
length(resid(linreg))
```


### Linear Regression with Regularization

5. Create a linear model using L1 or L2 regularization to predict Temperature from Wind and Month variables. Plot your predicted values and the real Y values on the same plot. 
```{r}
set.seed(1234)
part2 <- sample(seq(nrow(airq)),size = smp_siz)
train_reg2 <- airq[part2,]
test_reg2 <- airq[-part2,]
temptest <- test_reg2$Temp
tempwind <- test_reg2$Wind
test <- cbind(temptest,tempwind)
fit2 <- lm(airq$Temp ~ airq$Wind)

lambda <- 10^seq(4, -1, by = -.1)
wind <- airq$Wind
temp <- airq$Temp
subtw <- cbind(wind,temp)
lasso.mod <- cv.glmnet(as.matrix(subtw), temp ,alpha = 1, lambda = lambda)
plot(lasso.mod)
best_lam <- lasso.mod$lambda.min
lasso.best<- glmnet(subtw,temp,alpha = 1, lambda = best_lam)


lasso <- as.data.frame(cbind(lasso.pred, test[,1]))
names(lasso)[names(lasso) == "1"] <- "predictions"
names(lasso)[names(lasso) == "V2"] <- "test"
lasso
ggplot(data = lasso ) +
  geom_point(aes(x=predictions, y = test)) + 
  # geom_line(data = lasso, aes(x = predictions , y = test, col = "predictions"))+
  # geom_line(data = lasso, aes(x = predictions, y = test,   col = "test"))+
  ggtitle("Lasso Regularization")


```


